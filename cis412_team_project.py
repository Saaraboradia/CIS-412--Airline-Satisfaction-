# -*- coding: utf-8 -*-
"""CIS412: Team Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b2TFJoYXW-_t9lSEaVoWMmhO7FxQfiMq
"""

import pandas as pd

df = pd.read_csv('/content/sample_data/train.csv')

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

# Quick look at data
print("Shape:", df.shape)
print(df.info())
print(df.describe())

# 4. Remove duplicates
duplicate_count = df.duplicated().sum()
print("Duplicate rows:", duplicate_count)
if duplicate_count > 0:
    df = df.drop_duplicates()
    print("Dropped duplicates. New shape:", df.shape)

# 5. Check missing values
missing_summary = df.isnull().sum()
print("Missing values per column:\n", missing_summary[missing_summary > 0])

# 6. Impute missing values
# Define numeric and categorical columns
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

# Example: fill numeric with median; categorical with mode
for col in numeric_cols:
    median_val = df[col].median()
    df[col].fillna(median_val, inplace=True)

for col in categorical_cols:
    mode_val = df[col].mode()[0]
    df[col].fillna(mode_val, inplace=True)

# 7. Encode categorical variables
# The columns 'Gender', 'Customer Type', 'Type of Travel', 'Class' appear to have already been one-hot encoded
# as evidenced by columns like 'Gender_Male', 'Customer Type_disloyal Customer', etc. already existing in the DataFrame.
# Therefore, we will encode the remaining categorical columns.
# Example: Gender, Customer Type, Type of Travel, Class
# cols_to_encode = ['Gender', 'Customer Type', 'Type of Travel', 'Class'] # These columns no longer exist
# df = pd.get_dummies(df, columns=cols_to_encode, drop_first=True)

# Encode any remaining object/category type columns (e.g., 'satisfaction')
for col in df.select_dtypes(include=['object', 'category']).columns:
    df = pd.get_dummies(df, columns=[col], drop_first=True)

# 8. Clean and transform specific numeric features
# e.g., negative delay minutes are nonsensical → set to 0
delay_cols = ['Departure Delay in Minutes', 'Arrival Delay in Minutes']
for col in delay_cols:
    df[col] = df[col].apply(lambda x: x if x >= 0 else 0)

# 9. Standardize/scale numeric features (optional but recommended for certain models)
scaler = StandardScaler()
# List of numeric features to scale
features_to_scale = ['Flight Distance', 'Age', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']
df[features_to_scale] = scaler.fit_transform(df[features_to_scale])

# 10. Final check and save cleaned dataset
print("After cleaning – any missing values left?", df.isnull().sum().sum())
cleaned_path = '/content/drive/MyDrive/your_folder/train_cleaned.csv'  # modify if desired

# Create the directory if it doesn't exist
import os
os.makedirs(os.path.dirname(cleaned_path), exist_ok=True)

df.to_csv(cleaned_path, index=False)
print("Cleaned dataset saved to:", cleaned_path)

"""# Task
To identify variables significantly impacting customer satisfaction by fitting a logistic regression model using `statsmodels` and displaying variables with a p-value less than 0.05, based on the preprocessed data in `df` from `"/content/sample_data/train.csv"`.

## Identify Target and Features

### Subtask:
Determine the target variable (likely 'satisfaction_satisfied' after one-hot encoding) and all feature columns from the DataFrame.

**Reasoning**:
The subtask is to define the target variable `y` as 'satisfaction_satisfied' and the feature variables `X` by dropping unnecessary columns from the DataFrame `df`.
"""

y = df['satisfaction_satisfied']
X = df.drop(columns=['Unnamed: 0', 'id', 'satisfaction_satisfied'])

print("Shape of features (X):", X.shape)
print("Shape of target (y):", y.shape)
print("First 5 rows of features (X):\n", X.head())
print("First 5 rows of target (y):\n", y.head())

"""## Add Constant to Features

### Subtask:
Add a constant column to the feature set, which is required for statsmodels to correctly calculate the intercept for the logistic regression model.

**Reasoning**:
To prepare the feature set for logistic regression with `statsmodels`, a constant term needs to be added to the `X` DataFrame to allow for the calculation of an intercept. This is done by importing `statsmodels.api` and using its `add_constant` function.
"""

import statsmodels.api as sm
X = sm.add_constant(X)
print("First 5 rows of features (X) with constant:\n", X.head())

"""## Fit Logistic Regression Model

### Subtask:
Use `statsmodels.api.Logit` to fit a logistic regression model, as this library provides p-values for the coefficients directly.

**Reasoning**:
Instantiate and fit a logistic regression model using `statsmodels.api.Logit` with the prepared target `y` and features `X`.
"""

model = sm.Logit(y, X)
model_results = model.fit()
print(model_results.summary())

"""**Reasoning**:
The previous cell failed because the feature DataFrame `X` contained boolean columns that `statsmodels` interprets as `object` dtype. These boolean columns need to be converted to integers (0 or 1) before fitting the logistic regression model.

# Task
Convert all boolean columns in the feature DataFrame `X` to integers (0 or 1) to make them compatible with `statsmodels`. Then, fit a logistic regression model using `statsmodels.api.Logit` with the updated `X` and target `y`, and finally, display the variables that have a p-value less than 0.05 from the model summary.

## Convert Boolean Columns to Numeric

### Subtask:
Convert all boolean columns in the feature DataFrame `X` to integers (0 or 1) to make them compatible with `statsmodels`.

**Reasoning**:
The previous code failed because `statsmodels` does not handle boolean columns directly. This step converts all boolean columns in `X` to integers (0 or 1), making them compatible with the `statsmodels.Logit` function.
"""

for col in X.select_dtypes(include='bool').columns:
    X[col] = X[col].astype(int)

print("First 5 rows of features (X) after boolean conversion:\n", X.head())

non_significant_variables = model_results.pvalues[model_results.pvalues >= 0.05]
print("\nVariables with p-value >= 0.05:\n", non_significant_variables.sort_values(ascending=False))

print("Variables with p-value < 0.05:\n", significant_variables.sort_values())

import matplotlib.pyplot as plt
import numpy as np

sorted_significant_variables = significant_variables.sort_values(ascending=False)

plt.figure(figsize=(12, 8))
plt.barh(sorted_significant_variables.index, sorted_significant_variables.values)
plt.xlabel('P-value (log scale)')
plt.xscale('log')
plt.ylabel('Variables')
plt.title('Significant Variables (p-value < 0.05) - Sorted')
plt.gca().invert_yaxis() # Invert y-axis to have the most significant at the top
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""**Reasoning**:
Now that the boolean columns in `X` have been converted to integers, I will fit the logistic regression model using `statsmodels.api.Logit` and then display the variables with a p-value less than 0.05.


"""

model = sm.Logit(y, X)
model_results = model.fit()

# Display summary statistics
print(model_results.summary())

# Get p-values and filter for significant ones (p < 0.05)
significant_variables = model_results.pvalues[model_results.pvalues < 0.05]
print("\nVariables with p-value < 0.05:\n", significant_variables.sort_values())

"""## Summary:

### Q&A
The variables that have a p-value less than 0.05 from the model summary are: `const`, `Online boarding`, `Customer Type_disloyal Customer`, `Type of Travel_Personal Travel`, `Checkin service`, `Inflight wifi service`, `Leg room service`, `On-board service`, `Class_Eco`, `Class_Eco Plus`, `Cleanliness`, `Departure/Arrival time convenient`, `Ease of Online booking`, `Age`, `Baggage handling`, `Inflight service`, `Arrival Delay in Minutes`, `Seat comfort`, `Departure Delay in Minutes`, `Inflight entertainment`, `Gate location`, and `Gender_Male`.

### Data Analysis Key Findings
*   All boolean columns in the feature DataFrame `X` (e.g., `Gender_Male`, `Customer Type_disloyal Customer`, `Type of Travel_Personal Travel`, `Class_Eco`, `Class_Eco Plus`) were successfully converted to integers (0 or 1) to ensure compatibility with `statsmodels`.
*   A logistic regression model was successfully fitted using `statsmodels.api.Logit` with the updated feature DataFrame `X` and target `y`.
*   A significant number of variables, including `Online boarding`, `Customer Type_disloyal Customer`, `Type of Travel_Personal Travel`, `Checkin service`, `Inflight wifi service`, `Leg room service`, `On-board service`, `Class_Eco`, `Class_Eco Plus`, `Cleanliness`, `Departure/Arrival time convenient`, `Ease of Online booking`, `Age`, `Baggage handling`, `Inflight service`, `Arrival Delay in Minutes`, `Seat comfort`, `Departure Delay in Minutes`, `Inflight entertainment`, `Gate location`, and `Gender_Male`, were found to be statistically significant with a p-value less than 0.05.

### Insights or Next Steps
*   The identified significant variables indicate strong predictors for the target variable, which can be prioritized for further analysis or used to build more parsimonious predictive models.
*   Further investigation into the coefficients of these significant variables could reveal the direction and magnitude of their impact, providing deeper business insights into customer satisfaction or other relevant outcomes.

"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

"""# Task
Perform a logistic regression analysis on the preprocessed data from `'/content/sample_data/train.csv'` to identify variables impacting customer satisfaction, then extract and display both significant (p-value < 0.05) and non-significant (p-value >= 0.05) variables from the model summary.

## Display Non-Significant Variables

### Subtask:
Extract the p-values from the model summary and display the variables that have a p-value greater than or equal to 0.05.

## Summary:

### Data Analysis Key Findings
*   The current stage of the analysis focuses on identifying variables that do not significantly influence customer satisfaction.
*   Variables with a p-value greater than or equal to 0.05, derived from a logistic regression model, are categorized as non-significant.
*   The specific non-significant variables and their p-values have not yet been extracted or reported in the provided process.

### Insights or Next Steps
*   The immediate next step involves extracting p-values from the logistic regression model summary and explicitly displaying the variables identified as non-significant.
*   Identifying these non-significant variables is crucial for understanding which features might be considered for removal or further investigation in future model refinement.
"""

selected_columns = [
    'Checkin service', 'Food and drink', 'Inflight service',
    'Cleanliness', 'On-board service', 'Baggage handling', 'satisfaction_satisfied'
]

df_copy = df[selected_columns].copy()

df_copy['satisfaction_satisfied'] = df_copy['satisfaction_satisfied'].astype(int)

print("First 5 rows of the new DataFrame df_copy:\n", df_copy.head())
print("Data types of df_copy:\n", df_copy.info())

selected_columns = [
    'Checkin service', 'Food and drink', 'Inflight service',
    'Cleanliness', 'On-board service', 'Baggage handling', 'satisfaction_satisfied'
]

df_copy = df[selected_columns].copy()
df_copy['satisfaction_satisfied'] = df_copy['satisfaction_satisfied'].astype(int)

import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

# Define features (X) and target (y)
X = df_copy.drop(columns=['satisfaction_satisfied'])
y = df_copy['satisfaction_satisfied']

# Ensure all X variables are numeric
X = X.apply(pd.to_numeric)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Add constant term for intercept
X_train_const = sm.add_constant(X_train)
X_test_const = sm.add_constant(X_test, has_constant='add')

# Fit logistic regression model
logit_model = sm.Logit(y_train, X_train_const)
logit_results = logit_model.fit()

# Model summary
print(logit_results.summary())

# Extract p-values
pvalues = logit_results.pvalues

# Significant variables (p < 0.05)
significant_vars = pvalues[pvalues < 0.05].sort_values()
print("\nSignificant variables (p < 0.05):")
print(significant_vars)

# Non-significant variables (p >= 0.05)
nonsignificant_vars = pvalues[pvalues >= 0.05].sort_values(ascending=False)
print("\nNon-significant variables (p >= 0.05):")
print(nonsignificant_vars)

# Predictions
y_pred_prob = logit_results.predict(X_test_const)
y_pred = (y_pred_prob >= 0.5).astype(int)

# Evaluation
print("\nAccuracy on test set: {:.3f}".format(accuracy_score(y_test, y_pred)))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=3))

# AUC Score
auc = roc_auc_score(y_test, y_pred_prob)
print("ROC-AUC Score: {:.3f}".format(auc))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Ensure X_test_const has the constant term for prediction
# This line is not needed if X_test_const is already defined correctly
# X_test_copy_const = sm.add_constant(X_test_copy, has_constant='add') # Original line commented out

# Make predictions on the test set using the model trained on df_copy
y_pred_prob = logit_results.predict(X_test_const)
y_pred = (y_pred_prob >= 0.5).astype(int)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0 (Neutral/Dissatisfied)', 'Predicted 1 (Satisfied)'],
            yticklabels=['Actual 0 (Neutral/Dissatisfied)', 'Actual 1 (Satisfied)'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Customer Satisfaction (df_copy)')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Get feature importances from the Random Forest model
feature_importances = rf_model.feature_importances_

# Create a Series for better visualization and sorting
features_df = pd.Series(feature_importances, index=X_train.columns)

# Sort the features by importance in descending order
sorted_features = features_df.sort_values(ascending=False)

print("Feature Importances for Random Forest Model:\n")
print(sorted_features)

# Optionally, visualize the feature importances
plt.figure(figsize=(10, 6))
sorted_features.plot(kind='barh')
plt.title('Random Forest Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis() # Invert y-axis to have most important at the top
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Get coefficients and p-values from the logistic regression model
coefficients = logit_results.params
p_values = logit_results.pvalues

# Filter for statistically significant variables (p-value < 0.05)
significant_coeffs = coefficients[p_values < 0.05]
significant_p_values = p_values[p_values < 0.05]

# Exclude 'const' from the significant variables for plotting
significant_coeffs = significant_coeffs.drop('const', errors='ignore')
significant_p_values = significant_p_values.drop('const', errors='ignore')

# Combine coefficients and p-values into a DataFrame for easier handling
coeff_df = pd.DataFrame({
    'Coefficient': significant_coeffs,
    'P-value': significant_p_values
})

# Sort by absolute coefficient value for importance visualization
coeff_df['Abs_Coefficient'] = coeff_df['Coefficient'].abs()
sorted_coeffs = coeff_df.sort_values(by='Abs_Coefficient', ascending=True)

print("Significant Coefficients for Logistic Regression Model (sorted by absolute value):\n")
print(sorted_coeffs[['Coefficient', 'P-value']])

# Plot the coefficients
plt.figure(figsize=(12, 8))
plt.barh(sorted_coeffs.index, sorted_coeffs['Coefficient'], color=np.where(sorted_coeffs['Coefficient'] > 0, 'skyblue', 'salmon'))
plt.xlabel('Coefficient Value')
plt.ylabel('Features')
plt.title('Significant Feature Coefficients for Logistic Regression Model (Excluding Constant)')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix for the Random Forest model
cm_rf = confusion_matrix(y_test, y_test_pred_rf)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0 (Neutral/Dissatisfied)', 'Predicted 1 (Satisfied)'],
            yticklabels=['Actual 0 (Neutral/Dissatisfied)', 'Actual 1 (Satisfied)'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Random Forest Model')
plt.show()

"""## Implement Random Forest Classifier

To potentially improve model performance and address underfitting, we will now implement a Random Forest Classifier. This ensemble method is known for its ability to handle complex relationships in data and often yields higher accuracy than simpler models like logistic regression.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score

# Initialize the Random Forest Classifier
# Using a reasonable number of estimators and a random state for reproducibility
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the training set
y_train_pred_rf = rf_model.predict(X_train)
y_train_prob_rf = rf_model.predict_proba(X_train)[:, 1]

# Make predictions on the test set
y_test_pred_rf = rf_model.predict(X_test)
y_test_prob_rf = rf_model.predict_proba(X_test)[:, 1]

# Evaluate Training Set Performance
train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)
train_roc_auc_rf = roc_auc_score(y_train, y_train_prob_rf)

print(f"Random Forest - Training Accuracy: {train_accuracy_rf:.3f}")
print(f"Random Forest - Training ROC-AUC: {train_roc_auc_rf:.3f}")

print("\n--- Random Forest - Test Set Evaluation ---")
print(f"Accuracy on test set: {accuracy_score(y_test, y_test_pred_rf):.3f}")

print("\nConfusion Matrix (Test Set):")
print(confusion_matrix(y_test, y_test_pred_rf))

print("\nClassification Report (Test Set):")
print(classification_report(y_test, y_test_pred_rf, digits=3))

print(f"ROC-AUC Score (Test Set): {roc_auc_score(y_test, y_test_prob_rf):.3f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
import statsmodels.api as sm

train_sizes = np.linspace(0.1, 1.0, 10)

train_accuracies = []
test_accuracies = []

for size in train_sizes:
    # Determine the number of samples for the current subset
    num_samples = int(size * len(X_train))

    # Select a subset of the training data
    X_train_subset = X_train.iloc[:num_samples]
    y_train_subset = y_train.iloc[:num_samples]

    # Add a constant to the subset of training data
    X_train_subset_const = sm.add_constant(X_train_subset)

    # Fit a new logistic regression model on the subset
    # Handle cases where the subset might be too small or cause issues
    try:
        logit_model_subset = sm.Logit(y_train_subset, X_train_subset_const)
        logit_results_subset = logit_model_subset.fit(disp=0) # disp=0 to suppress convergence messages

        # Calculate training accuracy for the subset
        y_train_pred_prob_subset = logit_results_subset.predict(X_train_subset_const)
        y_train_pred_subset = (y_train_pred_prob_subset >= 0.5).astype(int)
        train_accuracy_subset = accuracy_score(y_train_subset, y_train_pred_subset)
        train_accuracies.append(train_accuracy_subset)

        # Calculate test accuracy using the model trained on the subset
        y_test_pred_prob_subset = logit_results_subset.predict(X_test_const)
        y_test_pred_subset = (y_test_pred_prob_subset >= 0.5).astype(int)
        test_accuracy_subset = accuracy_score(y_test, y_test_pred_subset)
        test_accuracies.append(test_accuracy_subset)

    except Exception as e:
        print(f"Error fitting model with {num_samples} samples: {e}")
        train_accuracies.append(np.nan) # Append NaN if fitting fails
        test_accuracies.append(np.nan)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes * len(X_train), train_accuracies, label='Training Accuracy', marker='o')
plt.plot(train_sizes * len(X_train), test_accuracies, label='Test Accuracy', marker='o')
plt.xlabel('Number of Training Samples')
plt.ylabel('Accuracy')
plt.title('Learning Curve for Logistic Regression Model')
plt.legend()
plt.grid(True)
plt.show()

"""accuracy on training set and also accuracy on test set

show learning curve

plan to improve the model

show the code and what features used to encode

potential overfitting by showing training and test sets

# Task
Generate a learning curve for the logistic regression model by iteratively training it on increasing subsets of the training data (from 10% to 100% in 10% increments) and evaluating its accuracy on both the training and test sets. Finally, plot the learning curve to visualize the model's performance as the training data size increases.

## GenerateLearningCurve

### Subtask:
Generate a learning curve by iteratively training the logistic regression model on increasing subsets of the training data and evaluating its performance on both the training and test sets.

## Summary:

### Data Analysis Key Findings
*   The model's performance (accuracy) was assessed by iteratively training a logistic regression model on increasing subsets of the training data (from 10% to 100% in 10% increments).
*   Learning curves were generated to visualize the accuracy on both the training and test sets as the training data size increased.
*   The comparison of training and test accuracy curves provides insights into whether the model is underfitting (high bias), overfitting (high variance), or performing optimally.

### Insights or Next Steps
*   Analyze the generated learning curve to determine if the model suffers from high bias (underfitting) or high variance (overfitting), which would be indicated by the convergence behavior and the gap between the training and test accuracy curves.
*   Based on the learning curve analysis, decide on appropriate next steps, such as collecting more data (if the curves are still diverging and test accuracy is improving), simplifying the model (if overfitting), or increasing model complexity (if underfitting).

# Comparing Accuracy, precision and recall of logistic regression and random forest models
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score
import pandas as pd

# ---- Logistic Regression Metrics ----
log_acc = accuracy_score(y_test, y_pred)
log_precision = precision_score(y_test, y_pred)
log_recall = recall_score(y_test, y_pred)

# ---- Random Forest Metrics ----
rf_acc = accuracy_score(y_test, y_test_pred_rf)
rf_precision = precision_score(y_test, y_test_pred_rf)
rf_recall = recall_score(y_test, y_test_pred_rf)

# ---- Combine into a dataframe for clean comparison ----
results_df = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest'],
    'Accuracy': [log_acc, rf_acc],
    'Precision': [log_precision, rf_precision],
    'Recall': [log_recall, rf_recall]
})

print(results_df)

from sklearn.metrics import accuracy_score, precision_score, recall_score
import pandas as pd

# ----- Logistic Regression: train + test predictions -----
# Train
y_train_prob_log = logit_results.predict(X_train_const)
y_train_pred_log = (y_train_prob_log >= 0.5).astype(int)

# Test
y_test_prob_log = logit_results.predict(X_test_const)
y_test_pred_log = (y_test_prob_log >= 0.5).astype(int)

results = []

# Logistic Regression - Train
results.append({
    'Model': 'Logistic Regression',
    'Data Split': 'Train',
    'Accuracy': accuracy_score(y_train, y_train_pred_log),
    'Precision': precision_score(y_train, y_train_pred_log),
    'Recall': recall_score(y_train, y_train_pred_log)
})

# Logistic Regression - Test
results.append({
    'Model': 'Logistic Regression',
    'Data Split': 'Test',
    'Accuracy': accuracy_score(y_test, y_test_pred_log),
    'Precision': precision_score(y_test, y_test_pred_log),
    'Recall': recall_score(y_test, y_test_pred_log)
})

# Random Forest - Train
results.append({
    'Model': 'Random Forest',
    'Data Split': 'Train',
    'Accuracy': accuracy_score(y_train, y_train_pred_rf),
    'Precision': precision_score(y_train, y_train_pred_rf),
    'Recall': recall_score(y_train, y_train_pred_rf)
})

# Random Forest - Test
results.append({
    'Model': 'Random Forest',
    'Data Split': 'Test',
    'Accuracy': accuracy_score(y_test, y_test_pred_rf),
    'Precision': precision_score(y_test, y_test_pred_rf),
    'Recall': recall_score(y_test, y_test_pred_rf)
})

results_df_full = pd.DataFrame(results)
print(results_df_full)



"""# Task
Generate a learning curve for the Random Forest model by plotting the training and test accuracies against the number of training samples, and include a legend to distinguish between the two curves.

## Prepare for Learning Curve Generation

### Subtask:
Define the range for training data sizes and initialize lists to store training and test accuracies for the Random Forest model.

**Reasoning**:
Define the range for training data sizes, initialize lists for storing accuracies, and calculate the number of training samples for the Random Forest model's learning curve.
"""

import numpy as np

train_sizes = np.linspace(0.1, 1.0, 10)

train_accuracies_rf = []
test_accuracies_rf = []

# Calculate the actual number of training samples for each size
num_training_samples = (train_sizes * len(X_train))

print("Training data sizes (proportions):", train_sizes)
print("Number of training samples for each size:", num_training_samples)
print("Initialized empty lists for RF training and test accuracies.")

"""**Reasoning**:
To generate the learning curve, I will iterate through the defined training sizes, train a Random Forest model on each subset of the training data, and then calculate and store the training and test accuracies.


"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

for size in train_sizes:
    # Determine the number of samples for the current subset
    num_samples = int(size * len(X_train))

    # Select a subset of the training data
    X_train_subset = X_train.iloc[:num_samples]
    y_train_subset = y_train.iloc[:num_samples]

    # Initialize and train the Random Forest Classifier on the subset
    rf_model_subset = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf_model_subset.fit(X_train_subset, y_train_subset)

    # Calculate training accuracy for the subset
    y_train_pred_rf_subset = rf_model_subset.predict(X_train_subset)
    train_accuracy_rf_subset = accuracy_score(y_train_subset, y_train_pred_rf_subset)
    train_accuracies_rf.append(train_accuracy_rf_subset)

    # Calculate test accuracy using the model trained on the subset
    y_test_pred_rf_subset = rf_model_subset.predict(X_test)
    test_accuracy_rf_subset = accuracy_score(y_test, y_test_pred_rf_subset)
    test_accuracies_rf.append(test_accuracy_rf_subset)

print("Training accuracies for Random Forest:", train_accuracies_rf)
print("Test accuracies for Random Forest:", test_accuracies_rf)

"""**Reasoning**:
Now that the training and test accuracies for the Random Forest model have been calculated for various training data sizes, the next step is to plot these results to visualize the learning curve.


"""

import matplotlib.pyplot as plt

# Plot the learning curve for Random Forest
plt.figure(figsize=(10, 6))
plt.plot(num_training_samples, train_accuracies_rf, label='Training Accuracy', marker='o')
plt.plot(num_training_samples, test_accuracies_rf, label='Test Accuracy', marker='o')
plt.xlabel('Number of Training Samples')
plt.ylabel('Accuracy')
plt.title('Learning Curve for Random Forest Model')
plt.legend()
plt.grid(True)
plt.show()

"""## Summary:

### Data Analysis Key Findings
*   As the number of training samples increased, the Random Forest model's training accuracy slightly decreased, from approximately 0.828 to 0.785.
*   Conversely, the test accuracy for the Random Forest model generally showed an upward trend with more training data, increasing from approximately 0.744 to 0.761.
*   The generated learning curve visually demonstrates the relationship between training and test accuracies and the number of training samples, indicating that the model's generalization performance improves with more data.

### Insights or Next Steps
*   The learning curve suggests that the Random Forest model benefits from more training data, as indicated by the improving test accuracy. Further performance gains might be possible with even larger datasets.
*   The persistent gap between training and test accuracy, even with more data, indicates that the model might still be overfitting to some extent. Future steps could involve hyperparameter tuning or exploring regularization techniques to reduce this gap and improve generalization.

"""