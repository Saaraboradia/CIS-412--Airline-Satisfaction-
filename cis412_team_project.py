# app.py
# -*- coding: utf-8 -*-
"""
Streamlit app converted from CIS412_Team_Project.ipynb
This app includes all comment content from the original notebook, and displays the plots and outputs.
"""

import os
import io
import pandas as pd
import numpy as np
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report, roc_auc_score,
    precision_score, recall_score
)
from sklearn.ensemble import RandomForestClassifier

st.set_page_config(page_title="CIS412 - Customer Satisfaction Explorer", layout="wide")

# ---- Helper functions ----
def show_dataframe_info(df, name="DataFrame"):
    st.write(f"**Shape:** {df.shape}")
    buffer = io.StringIO()
    df.info(buf=buffer)
    s = buffer.getvalue()
    st.text(s)
    st.write("**Describe:**")
    st.dataframe(df.describe(include='all').T)

def plot_confusion_matrix(cm, xticklabels, yticklabels, title):
    fig, ax = plt.subplots(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=xticklabels, yticklabels=yticklabels, ax=ax)
    ax.set_xlabel('Predicted Label')
    ax.set_ylabel('True Label')
    ax.set_title(title)
    st.pyplot(fig)

def plot_barh_series(values, xlabel='Value', title=''):
    fig, ax = plt.subplots(figsize=(10, max(4, len(values)*0.3)))
    values.plot(kind='barh', ax=ax)
    ax.set_xlabel(xlabel)
    ax.set_title(title)
    ax.invert_yaxis()
    st.pyplot(fig)

def safe_add_constant(X, has_constant='add'):
    # statsmodels.add_constant uses 'has_constant' to avoid duplication; keep it safe
    return sm.add_constant(X, has_constant=has_constant)

# ---- App UI ----
st.title("CIS412 - Customer Satisfaction Explorer")
st.markdown("""
This Streamlit app reproduces and displays the content, comments, and plots from the original
`CIS412_Team_Project.ipynb` notebook. It performs preprocessing, logistic regression analysis
(using `statsmodels`), a Random Forest model, and visualizations including learning curves
and confusion matrices. All explanatory comments from the notebook are shown below as sections.
""")

# Data upload
st.sidebar.header("Data")
st.sidebar.write("Upload `train.csv` or the app will attempt to read `./train.csv` from the working directory.")
uploaded_file = st.sidebar.file_uploader("Upload train.csv (or place data/train.csv in repo)", type=["csv"])

if uploaded_file is not None:
    try:
        df = pd.read_csv(uploaded_file)
        st.success("Uploaded CSV read successfully.")
    except Exception as e:
        st.error(f"Failed to read uploaded CSV: {e}")
        st.stop()
else:
    # fallback
    fallback_paths = ["./train.csv", "./data/train.csv", "/content/sample_data/train.csv"]
    df = None
    for p in fallback_paths:
        if os.path.exists(p):
            try:
                df = pd.read_csv(p)
                st.info(f"Loaded data from {p}")
                break
            except Exception:
                continue
    if df is None:
        st.warning("No data found. Please upload train.csv using the uploader in the sidebar, or add data/train.csv to the repo.")
        st.stop()

st.header("Raw Data Overview")
st.write("Raw Data (first 5 rows):")
st.dataframe(df.head())

show_dataframe_info(df, name="Original df")

st.markdown("### Notebook comment block\n"
            "```python\n"
            "# -*- coding: utf-8 -*-\n"
            "\"\"\"CIS412_Team_Project.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    https://colab.research.google.com/github/Saaraboradia/CIS-412--Airline-Satisfaction-/blob/main/CIS412_Team_Project.ipynb\n\"\"\"\n```")

# 4. Remove duplicates
st.subheader("4. Remove duplicates")
duplicate_count = int(df.duplicated().sum())
st.write(f"Duplicate rows: {duplicate_count}")
if duplicate_count > 0:
    df = df.drop_duplicates()
    st.write(f"Dropped duplicates. New shape: {df.shape}")

# 5. Check missing values
st.subheader("5. Check missing values")
missing_summary = df.isnull().sum()
if (missing_summary > 0).any():
    st.write("Missing values per column (only >0 shown):")
    st.write(missing_summary[missing_summary > 0])
else:
    st.write("No missing values detected.")

# 6. Impute missing values
st.subheader("6. Impute missing values (numeric -> median, categorical -> mode)")
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

for col in numeric_cols:
    median_val = df[col].median()
    if pd.isna(median_val):
        continue
    df[col].fillna(median_val, inplace=True)

for col in categorical_cols:
    if df[col].mode().shape[0] > 0:
        mode_val = df[col].mode()[0]
        df[col].fillna(mode_val, inplace=True)

st.write("Imputation complete. Any missing values remaining?")
st.write(df.isnull().sum().sum())

# 7. Encode categorical variables
st.subheader("7. Encode categorical variables")
st.markdown("The notebook noted that 'Gender', 'Customer Type', 'Type of Travel', 'Class' appeared already one-hot encoded. "
            "We will get dummies for any remaining object/category columns.")
object_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
st.write("Object/category columns to convert to dummies:", object_cols)

if len(object_cols) > 0:
    df = pd.get_dummies(df, columns=object_cols, drop_first=True)
    st.write("One-hot encoding applied to remaining object/category columns.")
else:
    st.write("No object/category columns remain.")

st.write("Data shape after encoding:", df.shape)

# 8. Clean and transform specific numeric features
st.subheader("8. Clean and transform specific numeric features (negative delays -> 0)")
delay_cols = [c for c in ['Departure Delay in Minutes', 'Arrival Delay in Minutes'] if c in df.columns]
for col in delay_cols:
    df[col] = df[col].apply(lambda x: x if x >= 0 else 0)

# 9. Standardize/scale numeric features
st.subheader("9. Standardize/scale numeric features")
scaler = StandardScaler()
features_to_scale = [c for c in ['Flight Distance', 'Age', 'Departure Delay in Minutes', 'Arrival Delay in Minutes'] if c in df.columns]
if features_to_scale:
    try:
        df[features_to_scale] = scaler.fit_transform(df[features_to_scale])
        st.write("Scaled features:", features_to_scale)
    except Exception as e:
        st.warning(f"Scaling failed: {e}")
else:
    st.write("No matching numeric features to scale found.")

st.write("After cleaning – any missing values left?", int(df.isnull().sum().sum()))

# Save cleaned dataset locally (in working dir)
cleaned_path = os.path.join(os.getcwd(), "train_cleaned.csv")
try:
    df.to_csv(cleaned_path, index=False)
    st.write(f"Cleaned dataset saved to: {cleaned_path}")
except Exception as e:
    st.warning(f"Failed to save cleaned dataset: {e}")

# --- Task: identify target and features ---
st.header("Task: Identify Target and Features")
st.markdown("""
The notebook defined the target variable likely as `'satisfaction_satisfied'` after encoding.
We will attempt to find that column in the DataFrame and set `y = df['satisfaction_satisfied']`.
If the exact column is not present, we'll try to infer the target.
""")

if 'satisfaction_satisfied' not in df.columns:
    # try alternative: check columns containing 'satisfaction'
    sat_cols = [c for c in df.columns if 'satisfaction' in c.lower()]
    st.warning("Column 'satisfaction_satisfied' not found. Candidate satisfaction columns:")
    st.write(sat_cols)
    if len(sat_cols) == 1:
        target_col = sat_cols[0]
        st.info(f"Using {target_col} as target.")
    else:
        # attempt to find a binary satisfaction-like column (0/1)
        cand = None
        for c in sat_cols:
            if set(df[c].unique()) <= {0,1} or set(df[c].dropna().unique()) <= {0,1}:
                cand = c
                break
        if cand is not None:
            target_col = cand
            st.info(f"Using {target_col} as target (binary).")
        else:
            st.error("Cannot identify a satisfaction target column automatically. Please ensure 'satisfaction_satisfied' exists.")
            st.stop()
else:
    target_col = 'satisfaction_satisfied'

st.write("Target column selected:", target_col)
y = df[target_col].astype(int)  # ensure int

# Build features X by dropping obvious non-feature columns if present
drop_cols = []
for c in ['Unnamed: 0', 'id', 'satisfaction_satisfied', target_col]:
    if c in df.columns and c not in drop_cols:
        drop_cols.append(c)
X = df.drop(columns=[c for c in drop_cols if c in df.columns])

st.write("Shape of features (X):", X.shape)
st.write("Shape of target (y):", y.shape)
st.write("First 5 rows of features (X):")
st.dataframe(X.head())
st.write("First 5 rows of target (y):")
st.dataframe(y.head())

st.subheader("Add constant to features (for statsmodels)")
X_const = safe_add_constant(X)
st.write("First 5 rows of features (X) with constant:")
st.dataframe(X_const.head())

# Convert boolean columns to integers
st.subheader("Convert boolean columns in features to integers (0/1)")
bool_cols = X_const.select_dtypes(include='bool').columns.tolist()
if bool_cols:
    st.write("Boolean columns converted:", bool_cols)
    for col in bool_cols:
        X_const[col] = X_const[col].astype(int)
else:
    st.write("No boolean columns found in feature set.")

# statsmodels requires numeric dtypes; ensure all columns numeric
non_numeric = X_const.select_dtypes(exclude=[np.number]).columns.tolist()
if non_numeric:
    st.warning("Non-numeric columns detected and will be converted where possible: " + ", ".join(non_numeric))
    for c in non_numeric:
        try:
            X_const[c] = pd.to_numeric(X_const[c])
        except Exception:
            st.error(f"Cannot convert column {c} to numeric. Dropping it.")
            X_const.drop(columns=[c], inplace=True)

# Fit logistic regression using statsmodels.Logit and display results
st.header("Fit Logistic Regression Model (statsmodels.Logit)")
try:
    model = sm.Logit(y, X_const)
    model_results = model.fit(disp=False)
    st.subheader("Model Summary")
    st.text(model_results.summary().as_text())
except Exception as e:
    st.error(f"Logit model failed to fit: {e}")
    # try a simpler modeling approach by removing perfectly collinear columns via regression' rank', or use a small set
    st.info("Attempting to drop columns with zero variance or high collinearity and retry.")
    try:
        # Drop zero-variance columns
        nzv = [c for c in X_const.columns if X_const[c].nunique() <= 1]
        if nzv:
            st.write("Dropping zero-variance columns:", nzv)
            X_const = X_const.drop(columns=nzv)
        model = sm.Logit(y, X_const)
        model_results = model.fit(disp=False)
        st.subheader("Model Summary (retry)")
        st.text(model_results.summary().as_text())
    except Exception as e2:
        st.error(f"Retry failed: {e2}")
        st.stop()

# Display p-values, significant/non-significant variables
st.subheader("P-values and Significance")
pvalues = model_results.pvalues
significant = pvalues[pvalues < 0.05].sort_values()
nonsignificant = pvalues[pvalues >= 0.05].sort_values(ascending=False)

st.write("Variables with p-value < 0.05 (significant):")
st.dataframe(significant.to_frame(name='p-value'))

st.write("Variables with p-value >= 0.05 (non-significant):")
st.dataframe(nonsignificant.to_frame(name='p-value'))

# Plot significant p-values (log scale)
if not significant.empty:
    st.subheader("Significant Variables (p < 0.05) - p-values (log scale)")
    sorted_sig = significant.sort_values(ascending=False)
    fig, ax = plt.subplots(figsize=(10, max(4, len(sorted_sig)*0.25)))
    ax.barh(sorted_sig.index, sorted_sig.values)
    ax.set_xscale('log')
    ax.set_xlabel('P-value (log scale)')
    ax.set_title('Significant Variables (p-value < 0.05) - Sorted')
    ax.invert_yaxis()
    ax.grid(axis='x', linestyle='--', alpha=0.7)
    st.pyplot(fig)
else:
    st.write("No significant variables to plot.")

# Provide the notebook's claimed summary as markdown (preserve comments)
st.markdown("### Notebook Summary (original comments)\n"
            "The notebook reported a list of significant variables including `const`, `Online boarding`, "
            "`Customer Type_disloyal Customer`, `Type of Travel_Personal Travel`, `Checkin service`, "
            "`Inflight wifi service`, `Leg room service`, `On-board service`, `Class_Eco`, `Class_Eco Plus`, "
            "`Cleanliness`, `Departure/Arrival time convenient`, `Ease of Online booking`, `Age`, "
            "`Baggage handling`, `Inflight service`, `Arrival Delay in Minutes`, `Seat comfort`, "
            "`Departure Delay in Minutes`, `Inflight entertainment`, `Gate location`, and `Gender_Male`."
            "\n\nIt also described next steps and how these variables could be used for further analysis.")

# Proceed with selected df_copy analysis from notebook
st.header("Selected variable subset analysis (df_copy) - logistic regression and evaluation")

selected_columns = [
    'Checkin service', 'Food and drink', 'Inflight service',
    'Cleanliness', 'On-board service', 'Baggage handling', target_col
]
# ensure availability
selected_columns = [c for c in selected_columns if c in df.columns or c == target_col]
if target_col not in selected_columns:
    selected_columns.append(target_col)

df_copy = df[selected_columns].copy()
df_copy[target_col] = df_copy[target_col].astype(int)

st.write("First 5 rows of df_copy:")
st.dataframe(df_copy.head())
buffer = io.StringIO()
df_copy.info(buf=buffer)
st.text(buffer.getvalue())

# Define X and y for df_copy
X_small = df_copy.drop(columns=[target_col])
y_small = df_copy[target_col]

# Ensure numeric
X_small = X_small.apply(pd.to_numeric, errors='coerce').fillna(0)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_small, y_small, test_size=0.2, random_state=42, stratify=y_small
)

# Add constant
X_train_const = safe_add_constant(X_train)
X_test_const = safe_add_constant(X_test)

st.write("Shapes - X_train:", X_train.shape, "X_test:", X_test.shape)
# Fit logistic regression on df_copy
st.subheader("Logistic Regression on selected features (df_copy)")
try:
    logit_model = sm.Logit(y_train, X_train_const)
    logit_results = logit_model.fit(disp=False)
    st.text(logit_results.summary().as_text())
except Exception as e:
    st.error(f"Failed to fit logistic model on selected features: {e}")
    st.stop()

# p-values from this model
pvalues_small = logit_results.pvalues
significant_vars = pvalues_small[pvalues_small < 0.05].sort_values()
nonsignificant_vars = pvalues_small[pvalues_small >= 0.05].sort_values(ascending=False)

st.write("Significant variables (p < 0.05):")
st.dataframe(significant_vars.to_frame(name='p-value'))

st.write("Non-significant variables (p >= 0.05):")
st.dataframe(nonsignificant_vars.to_frame(name='p-value'))

# Predictions and evaluation
y_pred_prob = logit_results.predict(X_test_const)
y_pred = (y_pred_prob >= 0.5).astype(int)

st.write("Accuracy on test set (logistic):", accuracy_score(y_test, y_pred))
st.write("Confusion Matrix (logistic):")
cm = confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cm,
                      xticklabels=['Predicted 0 (Neutral/Dissatisfied)', 'Predicted 1 (Satisfied)'],
                      yticklabels=['Actual 0 (Neutral/Dissatisfied)', 'Actual 1 (Satisfied)'],
                      title='Confusion Matrix for Customer Satisfaction (df_copy)')

st.write("Classification Report (logistic):")
st.text(classification_report(y_test, y_pred, digits=3))

auc = roc_auc_score(y_test, y_pred_prob)
st.write(f"ROC-AUC Score (logistic): {auc:.3f}")

# Random Forest on the same small dataset
st.subheader("Random Forest Classifier (on df_copy)")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)

y_train_pred_rf = rf_model.predict(X_train)
y_train_prob_rf = rf_model.predict_proba(X_train)[:, 1]
y_test_pred_rf = rf_model.predict(X_test)
y_test_prob_rf = rf_model.predict_proba(X_test)[:, 1]

st.write(f"Random Forest - Training Accuracy: {accuracy_score(y_train, y_train_pred_rf):.3f}")
st.write(f"Random Forest - Training ROC-AUC: {roc_auc_score(y_train, y_train_prob_rf):.3f}")
st.write(f"Random Forest - Test Accuracy: {accuracy_score(y_test, y_test_pred_rf):.3f}")
st.write("Random Forest - Confusion Matrix (Test):")
cm_rf = confusion_matrix(y_test, y_test_pred_rf)
plot_confusion_matrix(cm_rf,
                      xticklabels=['Predicted 0 (Neutral/Dissatisfied)', 'Predicted 1 (Satisfied)'],
                      yticklabels=['Actual 0 (Neutral/Dissatisfied)', 'Actual 1 (Satisfied)'],
                      title='Confusion Matrix for Random Forest Model')

st.write("Random Forest - Classification Report (Test):")
st.text(classification_report(y_test, y_test_pred_rf, digits=3))
st.write(f"Random Forest - ROC-AUC (Test): {roc_auc_score(y_test, y_test_prob_rf):.3f}")

# Feature importances from RF (on df_copy)
st.subheader("Random Forest Feature Importances")
feature_importances = rf_model.feature_importances_
features_df = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False)
st.dataframe(features_df.to_frame(name='importance'))

plot_barh_series(features_df, xlabel='Importance', title='Random Forest Feature Importances')

# Significant coefficients for logistic regression (df_copy)
st.subheader("Significant Coefficients from Logistic Regression (df_copy)")
coefficients = logit_results.params
p_values = logit_results.pvalues
significant_coeffs = coefficients[p_values < 0.05].drop('const', errors='ignore')
significant_p_values = p_values[p_values < 0.05].drop('const', errors='ignore')

coeff_df = pd.DataFrame({
    'Coefficient': significant_coeffs,
    'P-value': significant_p_values
})
coeff_df['Abs_Coefficient'] = coeff_df['Coefficient'].abs()
sorted_coeffs = coeff_df.sort_values(by='Abs_Coefficient', ascending=True)

if not sorted_coeffs.empty:
    st.dataframe(sorted_coeffs[['Coefficient', 'P-value']])
    # Plot
    fig, ax = plt.subplots(figsize=(10, max(3, len(sorted_coeffs)*0.3)))
    colors = ['#5DA5FF' if v>0 else '#FF6F6F' for v in sorted_coeffs['Coefficient']]
    ax.barh(sorted_coeffs.index, sorted_coeffs['Coefficient'], color=colors)
    ax.set_xlabel('Coefficient Value')
    ax.set_title('Significant Feature Coefficients for Logistic Regression Model (Excluding Constant)')
    ax.grid(axis='x', linestyle='--', alpha=0.7)
    st.pyplot(fig)
else:
    st.write("No significant coefficients (excluding constant) to display.")

# Learning curve for logistic regression (varying training size)
st.header("Learning Curve - Logistic Regression (on df_copy)")
train_sizes = np.linspace(0.1, 1.0, 10)
train_accuracies = []
test_accuracies = []

for size in train_sizes:
    num_samples = max(2, int(size * len(X_train)))  # ensure at least 2 samples
    X_train_subset = X_train.iloc[:num_samples]
    y_train_subset = y_train.iloc[:num_samples]
    X_train_subset_const = safe_add_constant(X_train_subset)
    try:
        logit_subset = sm.Logit(y_train_subset, X_train_subset_const)
        res_subset = logit_subset.fit(disp=False)
        y_train_pred_prob_subset = res_subset.predict(X_train_subset_const)
        y_train_pred_subset = (y_train_pred_prob_subset >= 0.5).astype(int)
        train_accuracy_subset = accuracy_score(y_train_subset, y_train_pred_subset)
        train_accuracies.append(train_accuracy_subset)

        y_test_pred_prob_subset = res_subset.predict(X_test_const)
        y_test_pred_subset = (y_test_pred_prob_subset >= 0.5).astype(int)
        test_accuracy_subset = accuracy_score(y_test, y_test_pred_subset)
        test_accuracies.append(test_accuracy_subset)
    except Exception as e:
        # append nan when fitting fails
        train_accuracies.append(np.nan)
        test_accuracies.append(np.nan)

num_training_samples = (train_sizes * len(X_train)).astype(int)
fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(num_training_samples, train_accuracies, label='Training Accuracy', marker='o')
ax.plot(num_training_samples, test_accuracies, label='Test Accuracy', marker='o')
ax.set_xlabel('Number of Training Samples')
ax.set_ylabel('Accuracy')
ax.set_title('Learning Curve for Logistic Regression Model')
ax.legend()
ax.grid(True)
st.pyplot(fig)

st.markdown("""
**Interpretation hints (from notebook)**:
* The model's performance (accuracy) was assessed by iteratively training a logistic regression model on increasing subsets of the training data (from 10% to 100% in 10% increments).
* The comparison of training and test accuracy curves provides insights into whether the model suffers from high bias (underfitting), high variance (overfitting), or is performing well.
""")

# Learning curve for Random Forest
st.header("Learning Curve - Random Forest (on df_copy)")
train_accuracies_rf = []
test_accuracies_rf = []

for size in train_sizes:
    num_samples = max(2, int(size * len(X_train)))
    X_train_subset = X_train.iloc[:num_samples]
    y_train_subset = y_train.iloc[:num_samples]

    rf_subset = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf_subset.fit(X_train_subset, y_train_subset)

    y_train_pred_rf_subset = rf_subset.predict(X_train_subset)
    train_accuracy_rf_subset = accuracy_score(y_train_subset, y_train_pred_rf_subset)
    train_accuracies_rf.append(train_accuracy_rf_subset)

    y_test_pred_rf_subset = rf_subset.predict(X_test)
    test_accuracy_rf_subset = accuracy_score(y_test, y_test_pred_rf_subset)
    test_accuracies_rf.append(test_accuracy_rf_subset)

fig, ax = plt.subplots(figsize=(10, 6))
ax.plot(num_training_samples, train_accuracies_rf, label='Training Accuracy', marker='o')
ax.plot(num_training_samples, test_accuracies_rf, label='Test Accuracy', marker='o')
ax.set_xlabel('Number of Training Samples')
ax.set_ylabel('Accuracy')
ax.set_title('Learning Curve for Random Forest Model')
ax.legend()
ax.grid(True)
st.pyplot(fig)

st.markdown("""
**Notebook Summary & Next Steps**
* The learning curves help diagnose underfitting vs overfitting. If training accuracy is high and test accuracy is low → overfitting.
* Next steps could include hyperparameter tuning for Random Forest, regularization for logistic regression, feature selection, or collecting more data.
""")

# Compare accuracy, precision, recall of logistic regression and random forest
st.header("Compare Logistic Regression vs Random Forest (accuracy, precision, recall)")

# Ensure we have predicted values for logistic from earlier (y_pred, y_pred_prob) and rf (y_test_pred_rf)
# compute metrics
log_acc = accuracy_score(y_test, y_pred)
log_precision = precision_score(y_test, y_pred, zero_division=0)
log_recall = recall_score(y_test, y_pred, zero_division=0)

rf_acc = accuracy_score(y_test, y_test_pred_rf)
rf_precision = precision_score(y_test, y_test_pred_rf, zero_division=0)
rf_recall = recall_score(y_test, y_test_pred_rf, zero_division=0)

results_df = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest'],
    'Accuracy': [log_acc, rf_acc],
    'Precision': [log_precision, rf_precision],
    'Recall': [log_recall, rf_recall]
})

st.dataframe(results_df)

# Full train/test breakdown
results = []

# Logistic Regression - Train and Test
y_train_prob_log = logit_results.predict(X_train_const)
y_train_pred_log = (y_train_prob_log >= 0.5).astype(int)
y_test_prob_log = logit_results.predict(X_test_const)
y_test_pred_log = (y_test_prob_log >= 0.5).astype(int)

results.append({
    'Model': 'Logistic Regression',
    'Data Split': 'Train',
    'Accuracy': accuracy_score(y_train, y_train_pred_log),
    'Precision': precision_score(y_train, y_train_pred_log, zero_division=0),
    'Recall': recall_score(y_train, y_train_pred_log, zero_division=0)
})
results.append({
    'Model': 'Logistic Regression',
    'Data Split': 'Test',
    'Accuracy': accuracy_score(y_test, y_test_pred_log),
    'Precision': precision_score(y_test, y_test_pred_log, zero_division=0),
    'Recall': recall_score(y_test, y_test_pred_log, zero_division=0)
})

# Random Forest - Train and Test
results.append({
    'Model': 'Random Forest',
    'Data Split': 'Train',
    'Accuracy': accuracy_score(y_train, y_train_pred_rf),
    'Precision': precision_score(y_train, y_train_pred_rf, zero_division=0),
    'Recall': recall_score(y_train, y_train_pred_rf, zero_division=0)
})
results.append({
    'Model': 'Random Forest',
    'Data Split': 'Test',
    'Accuracy': accuracy_score(y_test, y_test_pred_rf),
    'Precision': precision_score(y_test, y_test_pred_rf, zero_division=0),
    'Recall': recall_score(y_test, y_test_pred_rf, zero_division=0)
})

results_df_full = pd.DataFrame(results)
st.dataframe(results_df_full)

st.markdown("""
## Final notes (from notebook)
This app attempted to reproduce the original workflow:
* Preprocessing (imputation, encoding, scaling).
* Logistic regression using statsmodels (so we can get p-values).
* Focused small-model logistic regression on selected features (df_copy).
* Random Forest classifier and comparison.
* Displayed all p-values, significant vs non-significant variables, coefficients, confusion matrices,
  ROC-AUC, and learning curves.

If anything fails (convergence or collinearity), the app reports the exception and attempts a conservative retry.
""")

st.success("Analysis complete. Explore the outputs and plots above.")
